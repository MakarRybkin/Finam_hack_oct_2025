import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, ConcatDataset
from sklearn.preprocessing import RobustScaler
import warnings
import os
import time
from tqdm import tqdm
import torch.nn.functional as F
import json
import re
import time
import json
from openai import (
    OpenAI,
    APIStatusError,
    APIConnectionError,
    RateLimitError,
    AuthenticationError
)

warnings.filterwarnings('ignore')

SEED = 42
CANDLES_PATH_1 = "/kaggle/input/finam-hackathon/candles.csv"
CANDLES_PATH_2 = "/kaggle/input/finam-hackathon/candles_2.csv"
NEWS_PATH_1 = "/kaggle/input/finam-hackathon/news.csv"
NEWS_PATH_2 = "/kaggle/input/finam-hackathon/news_2.csv"
TRAIN_CUTOFF = pd.Timestamp('2025-09-08')
INPUT_WINDOW = 60
PRED_HORIZON = 20
INPUT_DIM = 21
OPENROUTE_API_KEY = "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"

np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("\n 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")


def load_and_merge_data():
    candles_1 = pd.read_csv(CANDLES_PATH_1)
    candles_2 = pd.read_csv(CANDLES_PATH_2)
    candles_df = pd.concat([candles_1, candles_2], ignore_index=True)
    news_1 = pd.read_csv(NEWS_PATH_1)
    news_2 = pd.read_csv(NEWS_PATH_2)
    news_df = pd.concat([news_1, news_2], ignore_index=True)
    if 'begin' in candles_df.columns:
        candles_df["begin"] = pd.to_datetime(candles_df["begin"])
        candles_df = candles_df.sort_values(['ticker', 'begin']).reset_index(drop=True)
    if 'publish_date' in news_df.columns:
        news_df["publish_date"] = pd.to_datetime(news_df["publish_date"])

    return candles_df, news_df


CANDLES, NEWS = load_and_merge_data()
TRAIN_TICKERS = CANDLES[CANDLES['begin'] <= TRAIN_CUTOFF]['ticker'].unique()
TICKER_KEYWORDS = {
    "GAZP": ["–ì–∞–∑–ø—Ä–æ–º", "Gazprom", "–ì–ê–ó–ü", "Gazprom Neft", "–ù–µ—Ñ—Ç—å", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SBER": ["–°–±–µ—Ä–±–∞–Ω–∫", "Sberbank", "–°–ë–ï–†", "–°–±–µ—Ä", "–ë–∞–Ω–∫", "–ö–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SBERP": ["–°–±–µ—Ä–±–∞–Ω–∫-–ø", "Sberbank-p", "–°–ë–ï–†-–ø", "–ë–∞–Ω–∫", "–ö–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "LKOH": ["–õ—É–∫–æ–π–ª", "Lukoil", "–õ–£–ö–û–ô–õ", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "GMKN": ["–ù–æ—Ä–Ω–∏–∫–µ–ª—å", "Nornickel", "–ì–ú–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "YNDX": ["–Ø–Ω–¥–µ–∫—Å", "Yandex", "YNDX", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "VTBR": ["–í–¢–ë", "VTB", "–í–¢–ë –ë–∞–Ω–∫", "–ë–∞–Ω–∫", "–ö–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "ROSN": ["–†–æ—Å–Ω–µ—Ñ—Ç—å", "Rosneft", "–†–û–°–ù–ï–§–¢–¨", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "NVTK": ["–ù–û–í–ê–¢–≠–ö", "Novatek", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SIBN": ["–ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å", "Gazprom Neft", "–ì–ê–ó–ü–†–û–ú –ù–ï–§–¢–¨", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "PHOR": ["–§–æ—Å–ê–≥—Ä–æ", "PhosAgro", "–§–û–°–ê–ì–†–û", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "PLZL": ["–ü–æ–ª—é—Å –ó–æ–ª–æ—Ç–æ", "Polyus", "–ü–û–õ–Æ–°", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MTSS": ["–ú–¢–°", "MTS", "–ú–¢–° –ë–∞–Ω–∫", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MGNT": ["–ú–∞–≥–Ω–∏—Ç", "Magnit", "–ú–ê–ì–ù–ò–¢", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "CHMF": ["–°–µ–≤–µ—Ä—Å—Ç–∞–ª—å", "Severstal", "–°–ï–í–ï–†–°–¢–ê–õ–¨", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "NLMK": ["–ù–õ–ú–ö", "NLMK", "–ù–æ–≤–æ–ª–∏–ø–µ—Ü–∫–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–±–∏–Ω–∞—Ç", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "ALRS": ["–ê–õ–†–û–°–ê", "Alrosa", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "RUAL": ["–†–£–°–ê–õ", "Rusal", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "AFKS": ["–ê–§–ö –°–∏—Å—Ç–µ–º–∞", "AFK Sistema", "–°–ò–°–¢–ï–ú–ê", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MOEX": ["–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞", "MOEX", "–ë–ò–†–ñ–ê", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "HYDR": ["–†—É—Å–ì–∏–¥—Ä–æ", "RusHydro", "–†–£–°–ì–ò–î–†–û", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "IRAO": ["–ò–Ω—Ç–µ—Ä –†–ê–û", "Inter RAO", "–ò–ù–¢–ï–† –†–ê–û", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "AFLT": ["–ê—ç—Ä–æ—Ñ–ª–æ—Ç", "Aeroflot", "–ê–≠–†–û–§–õ–û–¢", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SNGS": ["–°—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑", "Surgutneftegas", "–°–ù–ì", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SNGSP": ["–°—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑-–ø", "Surgutneftegas-p", "–°–ù–ì-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "TATN": ["–¢–∞—Ç–Ω–µ—Ñ—Ç—å", "Tatneft", "–¢–ê–¢–ù–ï–§–¢–¨", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "TATNP": ["–¢–∞—Ç–Ω–µ—Ñ—Ç—å-–ø", "Tatneft-p", "–¢–ê–¢–ù–ï–§–¢–¨-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "VTBRP": ["–í–¢–ë-–ø", "VTB-p", "–í–¢–ë –ë–∞–Ω–∫-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MGNP": ["–ú–∞–≥–Ω–∏—Ç-–ø", "Magnit-p", "–ú–ê–ì–ù–ò–¢-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "TRNFP": ["–¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å", "Transneft", "–¢–†–ê–ù–°–ù–ï–§–¢–¨", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "TRNF": ["–¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å", "Transneft", "–¢–†–ê–ù–°–ù–ï–§–¢–¨", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "URKA": ["–£—Ä–∞–ª–∫–∞–ª–∏–π", "Uralkali", "–£–†–ê–õ–ö–ê–õ–ò–ô", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "URKAP": ["–£—Ä–∞–ª–∫–∞–ª–∏–π-–ø", "Uralkali-p", "–£–†–ê–õ–ö–ê–õ–ò–ô-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "BANE": ["–ë–∞—à–Ω–µ—Ñ—Ç—å", "Bashneft", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "FESH": ["–î–í–ú–ü", "–î–∞–ª—å–Ω–µ–≤–æ—Å—Ç–æ—á–Ω–æ–µ –º–æ—Ä—Å–∫–æ–µ –ø–∞—Ä–æ—Ö–æ–¥—Å—Ç–≤–æ", "FESCO", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "UWGN": ["–û–í–ö", "–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –≤–∞–≥–æ–Ω–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è", "–û–í–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "T": ["–¢–∏–Ω—å–∫–æ—Ñ—Ñ –ë–∞–Ω–∫", "Tinkoff", "–¢–ò–ù–¨–ö–û–§–§", "–¢-–ë–∞–Ω–∫", "–¢", "–ë–∞–Ω–∫", "–ö–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞", "IT", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "VKCO": ["VK Company", "VK", "–í–ö–û–ù–¢–ê–ö–¢–ï", "IT", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "PIKK": ["–ü–ò–ö-—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫", "PIK", "–ü–ò–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SMLT": ["–°–∞–º–æ–ª–µ—Ç", "Samolet", "–°–ê–ú–û–õ–ï–¢", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "POSI": ["Positive Technologies", "–ü–û–ó–ò–¢–ò–í", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "FIVE": ["X5 Retail Group", "X5", "–•5", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SVCB": ["–°–æ–≤–∫–æ–º–±–∞–Ω–∫", "Sovcombank", "–°–û–í–ö–û–ú–ë–ê–ù–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SVCBP": ["–°–æ–≤–∫–æ–º–±–∞–Ω–∫-–ø", "Sovcombank-p", "–°–û–í–ö–û–ú–ë–ê–ù–ö-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "CIAN": ["–¶–ò–ê–ù", "CIAN Group", "–¶–ò–ê–ù –ì–†–£–ü–ü", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "HHRU": ["HeadHunter", "–•–µ–¥—Ö–∞–Ω—Ç–µ—Ä", "HH", "IT", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "DELI": ["–î–µ–ª–∏–º–æ–±–∏–ª—å", "Delimobil", "–î–ï–õ–ò–ú–û–ë–ò–õ–¨", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "DIAS": ["–î–∏–∞—Å–æ—Ñ—Ç", "Diasoft", "DIASOFT", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "EVI": ["–ï–≤—Ä–æ–ø–ª–∞–Ω", "Europlan", "–ï–í–†–û–ü–õ–ê–ù", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MTBC": ["–ú–¢–° –ë–∞–Ω–∫", "MTS Bank", "–ú–¢–°-–ë–ê–ù–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "OZON": ["–û–∑–æ–Ω", "Ozon Holdings", "OZON", "IT", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "DATA": ["–ê—Ä–µ–Ω–∞–¥–∞—Ç–∞", "Arenadata", "DATA", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SPBE": ["–°–ü–ë –ë–∏—Ä–∂–∞", "SPB Exchange", "–°–ü–ë", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "CBOM": ["–ú–ö–ë", "Moscow Credit Bank", "–ú–û–°–ö–û–í–°–ö–ò–ô –ö–†–ï–î–ò–¢–ù–´–ô –ë–ê–ù–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "PRMB": ["–ü—Ä–æ–º—Å–≤—è–∑—å–±–∞–Ω–∫", "PSB", "–ü–°–ë", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "ABIO": ["–ê—Å—Ç—Ä–∞", "–ì–ö –ê—Å—Ç—Ä–∞", "Astra Group", "ASTRA", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SOFL": ["–°–æ—Ñ—Ç–ª–∞–π–Ω", "Softline", "–°–û–§–¢–õ–ê–ô–ù", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "DELPO": ["–î–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—ã–π —Ä–∞—Å–ø–∏—Å–∫–∏", "–î–µ–ø–æ–∑–∏—Ç–∞—Ä–Ω—ã–µ —Ä–∞—Å–ø–∏—Å–∫–∏", "–ì–î–†", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "RASP": ["–†–∞—Å–ø–∞–¥—Å–∫–∞—è", "Raspadskaya", "–†–ê–°–ü–ê–î–°–ö–ê–Ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "POLY": ["–ü–æ–ª–∏–º–µ—Ç–∞–ª–ª", "Polymetal", "–ü–û–õ–ò–ú–ï–¢–ê–õ–õ", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "RUALP": ["–†–£–°–ê–õ-–ø", "Rusal-p"],
    "MAGN": ["–ú–ú–ö", "MMK", "–ú–∞–≥–Ω–∏—Ç–æ–≥–æ—Ä—Å–∫–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–±–∏–Ω–∞—Ç", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MSNG": ["–ú–µ—á–µ–ª", "Mechel", "–ú–ï–ß–ï–õ", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "TRMK": ["–¢–ú–ö", "–¢—Ä—É–±–Ω–∞—è –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∞—è –∫–æ–º–ø–∞–Ω–∏—è", "TMK", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "KMAZ": ["–ö–ê–ú–ê–ó", "KAMAZ", "–ö–ê–ú–ê–ó", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "VSMO": ["–í–°–ú–ü–û-–ê–≤–∏—Å–º–∞", "VSMPO-AVISMA", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "AKRN": ["–ê–∫—Ä–æ–Ω", "Acron", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "KZOS": ["–ö–∞–∑–∞–Ω—å–æ—Ä–≥—Å–∏–Ω—Ç–µ–∑", "Kazanorgsintez", "–ö–û–°", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "GCHE": ["–ì—Ä—É–ø–ø–∞ –ß–µ—Ä–∫–∏–∑–æ–≤–æ", "Cherkizovo Group", "–ß–ï–†–ö–ò–ó–û–í–û", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "RZSP": ["–†—É—Å–∞–≥—Ä–æ", "Rusagro", "–†–£–°–ê–ì–†–û", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "RZSPP": ["–†—É—Å–∞–≥—Ä–æ-–ø", "Rusagro-p", "–†–£–°–ê–ì–†–û-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "OKEY": ["–û'–ö–ï–ô", "O'KEY", "–û–ö–ï–ô", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "APTK": ["–ê–ø—Ç–µ—á–Ω–∞—è —Å–µ—Ç—å 36,6", "APTEKA 36,6", "–ê–ü–¢–ï–ö–ê", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "DIXY": ["–î–∏–∫—Å–∏ –ì—Ä—É–ø–ø", "DIXY Group", "–î–ò–ö–°–ò", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "ABRD": ["–ê–±—Ä–∞—É-–î—é—Ä—Å–æ", "Abrau-Durso", "–ê–ë–†–ê–£", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MRKC": ["–†–æ—Å—Å–µ—Ç–∏ –¶–µ–Ω—Ç—Ä", "MRSK Center", "–ú–†–°–ö –¶–ï–ù–¢–†–ê", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MRKU": ["–†–æ—Å—Å–µ—Ç–∏ –£—Ä–∞–ª", "MRSK Urala", "–ú–†–°–ö –£–†–ê–õ–ê", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MRKP": ["–†–æ—Å—Å–µ—Ç–∏ –¶–µ–Ω—Ç—Ä –∏ –ü—Ä–∏–≤–æ–ª–∂—å–µ", "MRSK Center & Privolzhye", "–ú–†–°–ö –¶–ò–ü", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MRKV": ["–†–æ—Å—Å–µ—Ç–∏ –í–æ–ª–≥–∞", "MRSK Volga", "–ú–†–°–ö –í–û–õ–ì–ê", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MSRS": ["–†–æ—Å—Å–µ—Ç–∏ –ú–æ—Å–∫–æ–≤—Å–∫–∏–π —Ä–µ–≥–∏–æ–Ω", "–ú–û–≠–°–ö", "MSRS", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MRKK": ["–†–æ—Å—Å–µ—Ç–∏ –°–µ–≤–µ—Ä–Ω—ã–π –ö–∞–≤–∫–∞–∑", "MRK-SK", "–†–û–°–°–ï–¢–ò –°–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MRKY": ["–†–æ—Å—Å–µ—Ç–∏ –Æ–≥", "MRK-Yug", "–†–û–°–°–ï–¢–ò –Æ–ì", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "MRKZ": ["–†–æ—Å—Å–µ—Ç–∏ –°–µ–≤–µ—Ä–æ-–ó–∞–ø–∞–¥", "MRSK SZ", "–ú–†–°–ö –°–ó", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "FEES": ["–§–°–ö –ï–≠–°", "FSK EES", "–§–°–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "OGKB": ["–û–ì–ö-2", "OGK-2", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "TGKA": ["–¢–ì–ö-1", "TGK-1", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "TGLD": ["–¢–ì–ö-1", "–¢–ì–ö-1", "TGC-1", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "TGKB": ["–¢–ì–ö-2", "TGK-2", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "TGKN": ["–¢–ì–ö-14", "TGK-14", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "BELU": ["–ë–µ–ª—É–≥–∞ –ì—Ä—É–ø–ø", "BELUGA Group", "–ë–ï–õ–£–ì–ê", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "ETLN": ["–≠—Ç–∞–ª–æ–Ω", "Etalon Group", "–≠–¢–ê–õ–û–ù", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "SGZH": ["–°–µ–≥–µ–∂–∞ –ì—Ä—É–ø–ø", "Segezha Group", "–°–ï–ì–ï–ñ–ê", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "LSRG": ["–õ–°–†", "LSR Group", "–ì—Ä—É–ø–ø–∞ –õ–°–†", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "GEMC": ["–ú–∞—Ç—å –∏ –¥–∏—Ç—è", "MD Medical Group", "–ú–ê–¢–¨ –ò –î–ò–¢–Ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "AQUA": ["–ê–∫–≤–∞–∫—É–ª—å—Ç—É—Ä–∞", "–†—É—Å–ê–∫–≤–∞–∫—É–ª—å—Ç—É—Ä–∞", "AQUA", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "RTKM": ["–†–æ—Å—Ç–µ–ª–µ–∫–æ–º", "Rostelecom", "–†–û–°–¢–ï–õ–ï–ö–û–ú", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "RTKMP": ["–†–æ—Å—Ç–µ–ª–µ–∫–æ–º-–ø", "Rostelecom pref", "–†–û–°–¢–ï–õ–ï–ö–û–ú-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "RENI": ["–†–µ–Ω–µ—Å—Å–∞–Ω—Å –°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "–†–ï–ù–ï–°–°–ê–ù–° –°–¢–†–ê–•–û–í–ê–ù–ò–ï", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "ZAYM": ["–ó–∞–π–º–µ—Ä", "Zaymer", "–ó–ê–ô–ú–ï–†", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "ZAYMP": ["–ó–∞–π–º–µ—Ä-–ø", "Zaymer-p", "–ó–ê–ô–ú–ï–†-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "PHST": ["–§–∞—Ä–º—Å–∏–Ω—Ç–µ–∑", "Pharmasynthez", "–§–ê–†–ú–°–ò–ù–¢–ï–ó", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "RSG": ["–†—É—Å–ì–∏–¥—Ä–æ", "RusHydro", "–†–£–°–ì–ò–î–†–û", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "LSNGP": ["–õ–µ–Ω—ç–Ω–µ—Ä–≥–æ-–ø", "Lenenergo-p", "–õ–ï–ù–≠–ù–≠–†–ì–û-–ø", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "GAZT": ["–ì–ê–ó-–¢–µ–∫", "GAZ-Tek", "–ì–ê–ó–¢–ï–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "IRKT": ["–ò—Ä–∫—É—Ç", "Irkut", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "UNAC": ["–û–ê–ö", "–û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–∞—è –∞–≤–∏–∞—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "NKNC": ["–ù–∏–∂–Ω–µ–∫–∞–º—Å–∫–Ω–µ—Ñ—Ç–µ—Ö–∏–º", "Nizhnekamskneftekhim", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "CHKZ": ["–ß–µ–ª—è–±–∏–Ω—Å–∫–∏–π –∫—É–∑–Ω–µ—á–Ω–æ-–ø—Ä–µ—Å—Å–æ–≤—ã–π –∑–∞–≤–æ–¥", "–ß–ö–ü–ó", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "CHMK": ["–ß–µ–ª—è–±–∏–Ω—Å–∫–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–±–∏–Ω–∞—Ç", "–ß–ú–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "ALNU": ["–ê–ª–º–∞–∑–Ω–∞—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å", "ALNU", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "OKB": ["–û–ö '–†—É—Å—å'", "OK", "OKB", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "ISKJ": ["–ì–ö –°–∞–º–æ–ª–µ—Ç", "Samolet Group", "–°–ê–ú–û–õ–ï–¢-–ò–°–ö", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"],
    "AKB": ["–ê–ö –ë–ê–†–°", "Ak Bars Bank", "–ê–ö–ë–ê–†–°", "–¢—Ä–∞–º–ø", "–ü—É—Ç–∏–Ω"]
}


def call_llm_for_features(news_text: str, ticker: str, api_key: str) -> tuple[int, int]:
    client = OpenAI(
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1"
    )
    system_prompt = (
        "You are an expert financial analyst. Analyze the following news article for "
        f"stock ticker {ticker}. Output the result strictly in JSON format: "
        '{"sentiment": N, "impact": M}, where N is the sentiment (integer from -3 to 3, '
        'and M is the market impact (integer from 0 to 5). '
        "Do not include any other text or explanation."
    )

    max_retries = 3
    for attempt in range(max_retries):
        try:
            print("\n" + "=" * 50)
            print(f"[{ticker}]  –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM:")
            print(news_text[:500] + ("..." if len(news_text) > 500 else ""))
            print("=" * 50)

            completion = client.chat.completions.create(
                extra_headers={"OpenRouter-App-Title": "Financial Feature Extractor"},
                model="openai/gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": news_text}
                ],
                response_format={"type": "json_object"},
                temperature=0.0
            )

            response_text = completion.choices[0].message.content
            data = json.loads(response_text)

            print(f"[{ticker}]  –û—Ç–≤–µ—Ç LLM:")
            print(json.dumps(data, indent=2, ensure_ascii=False))
            return data.get("sentiment", 0), data.get("impact", 0)

        except (APIStatusError, APIConnectionError, RateLimitError, json.JSONDecodeError) as e:
            print(f"[{ticker}]  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ API: {type(e).__name__} - {e}")
            if attempt < max_retries - 1:
                print(f"[{ticker}] –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ ({attempt + 2}/{max_retries})...")
                time.sleep(2 ** attempt)
                continue
            return 0, 0
        except Exception as e:
            print(f"[{ticker}]  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {type(e).__name__} - {e}")
            return 0, 0
    return 0, 0


class NewsFeatureExtractor:
    def __init__(self, news_df, candles_df, ticker_keywords, train_cutoff, api_key, max_api_calls=5000):
        self.news_df = news_df
        self.candles_df = candles_df
        self.ticker_keywords = ticker_keywords
        self.train_cutoff = train_cutoff
        self.api_key = api_key
        self.max_api_calls = max_api_calls
        self.llm_results = {}
        self.api_calls = 0
        self.log_records = []

    def extract_features(self, train_tickers):
        print(f"üìä –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Ç–∏–∫–µ—Ä–æ–≤ –∏–∑ TRAIN...")

        for ticker, keywords in self.ticker_keywords.items():
            if ticker not in train_tickers:
                continue

            if self.api_calls >= self.max_api_calls:
                print(" –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç API –≤—ã–∑–æ–≤–æ–≤")
                break

            keyword_pattern = '|'.join([re.escape(k) for k in keywords])
            news_filtered = self.news_df[
                self.news_df['title'].str.contains(keyword_pattern, case=False, na=False)
            ].copy()

            if news_filtered.empty:
                continue

            news_filtered.sort_values('publish_date', inplace=True, ascending=False)
            recent_news = news_filtered.head(30)

            results = []
            for _, row in recent_news.iterrows():
                if self.api_calls >= self.max_api_calls:
                    break

                news_text = f"Title: {row['title']}\nPublication: {row['publication']}"
                response = call_llm_for_features(news_text, ticker, self.api_key)
                self.api_calls += 1

                self.log_records.append({
                    'ticker': ticker,
                    'date': row['publish_date'],
                    'title': row['title'],
                    'publication': row['publication'],
                    'llm_response': response
                })

                if response:
                    results.append(response)

            if results:
                self.llm_results[ticker] = results

        if self.log_records:
            import pandas as pd
            pd.DataFrame(self.log_records).to_csv('llm_news_logs.csv', index=False, encoding='utf-8-sig')
            print(f" LLM –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π —Å–æ—Ö—Ä–∞–Ω—ë–Ω ({len(self.log_records)} –∑–∞–ø–∏—Å–µ–π)")

        return self.llm_results


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]


class TemporalConvNet(nn.Module):
    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.3):
        super().__init__()
        self.layers = nn.ModuleList()
        num_levels = len(num_channels)

        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i - 1]
            out_channels = num_channels[i]
            conv = nn.Conv1d(
                in_channels, out_channels, kernel_size,
                padding=(dilation_size * (kernel_size - 1)) // 2,
                dilation=dilation_size
            )
            self.layers.append(nn.Sequential(
                conv,
                nn.BatchNorm1d(out_channels),
                nn.ReLU(),
                nn.Dropout(dropout)
            ))

        self.output_channels = num_channels[-1]

    def forward(self, x):
        x = x.transpose(1, 2)
        for layer in self.layers:
            x = layer(x)
        return x.transpose(1, 2)


class ImprovedForecastModel(nn.Module):
    def __init__(self, input_dim=23, hidden_dim=128, num_layers=4,
                 num_heads=8, dropout=0.2, pred_horizon=20):
        super().__init__()

        self.pred_horizon = pred_horizon
        self.hidden_dim = hidden_dim

        self.feature_proj = nn.Linear(input_dim, hidden_dim)
        self.pos_encoder = PositionalEncoding(hidden_dim)
        self.tcn = TemporalConvNet(
            hidden_dim,
            [hidden_dim, hidden_dim, hidden_dim],
            kernel_size=7,
            dropout=dropout
        )
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            activation=F.gelu,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)
        self.final_repr_proj = nn.Linear(3 * hidden_dim, hidden_dim)

        self.return_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim // 2, 1)
            ) for _ in range(pred_horizon)
        ])

        self.prob_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim // 2, 1),
                nn.Sigmoid()
            ) for _ in range(pred_horizon)
        ])

        self.global_return_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, pred_horizon)
        )

    def forward(self, x):
        x = self.feature_proj(x)
        x = self.pos_encoder(x)

        tcn_out = self.tcn(x)
        trans_out = self.transformer(x)
        combined = tcn_out + trans_out + x
        attn_output, _ = self.attention(combined, combined, combined)
        last_state = attn_output[:, -1, :]
        avg_state = attn_output.mean(dim=1)
        max_state = attn_output.max(dim=1)[0]
        pooled = torch.cat([last_state, avg_state, max_state], dim=-1)
        final_repr = self.final_repr_proj(pooled)

        daily_returns = []
        daily_probs = []

        for i in range(self.pred_horizon):
            daily_returns.append(self.return_heads[i](final_repr))
            daily_probs.append(self.prob_heads[i](final_repr))

        daily_returns = torch.cat(daily_returns, dim=1)
        daily_probs = torch.cat(daily_probs, dim=1)
        global_returns = self.global_return_head(final_repr)
        final_returns = 0.7 * daily_returns + 0.3 * global_returns

        return final_returns, daily_probs


class ImprovedTimeSeriesDataset(Dataset):
    def __init__(self, df, llm_features_df, seq_len=60, pred_horizon=20,
                 feature_scaler=None, is_train=True, llm_weight=1):

        self.seq_len = seq_len
        self.pred_horizon = pred_horizon
        self.is_train = is_train
        self.llm_weight = llm_weight

        df_agg = df.groupby("begin").agg({
            "open": "first",
            "high": "max",
            "low": "min",
            "close": "last",
            "volume": "sum"
        }).reset_index()
        df_agg = df_agg.set_index("begin").asfreq("D").ffill().reset_index()

        # –°–¥–≤–∏–≥–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ +1 –¥–µ–Ω—å
        llm_features_df_shifted = llm_features_df.copy()
        llm_features_df_shifted['begin'] = llm_features_df_shifted['begin'] + pd.Timedelta(days=1)

        llm_features_df_shifted = llm_features_df_shifted.drop_duplicates(subset=['ticker', 'begin'])
        df_agg = pd.merge(df_agg, llm_features_df_shifted.drop(columns='ticker', errors='ignore'),
                          on='begin', how='left')

        llm_cols = [col for col in llm_features_df_shifted.columns if col not in ['ticker', 'begin']]
        for col in llm_cols:
            df_agg[col] = df_agg[col].fillna(0.0)

        # –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        self.features = self._create_features(df_agg, llm_cols)
        if is_train:
            self.targets = self._create_targets(df_agg)
        else:
            self.targets = None

        self.close_prices = df_agg['close'].values

        # –°–∫–µ–π–ª–µ—Ä
        if feature_scaler is None:
            self.feature_scaler = RobustScaler()
            if self.features.size > 0:
                self.features = self.feature_scaler.fit_transform(self.features)
        else:
            self.feature_scaler = feature_scaler
            if self.features.size > 0:
                self.features = self.feature_scaler.transform(self.features)

        self.valid_indices = self._get_valid_indices()

    def _create_features(self, df, llm_cols):
        features = []
        cols = ['open', 'high', 'low', 'close', 'volume']

        features.append(np.log(df['open']).values)
        features.append(np.log(df['high']).values)
        features.append(np.log(df['low']).values)
        features.append(np.log(df['close']).values)
        features.append(np.log(df['volume'] + 1).values)

        log_returns = np.log(df['close'] / df['close'].shift(1)).fillna(0)
        features.append(log_returns.values)
        features.append(log_returns.rolling(20).std().fillna(0).values)

        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        features.append(rsi.fillna(50).values / 100.0)

        ma5 = df['close'].rolling(5).mean()
        ma20 = df['close'].rolling(20).mean()
        features.append((ma5 / ma20 - 1).fillna(0).values)

        features.append((df['volume'] / df['volume'].shift(1) - 1).fillna(0).values)
        features.append(((df['high'] - df['low']) / df['close']).values)

        ema12 = df['close'].ewm(span=12, adjust=False).mean()
        ema26 = df['close'].ewm(span=26, adjust=False).mean()
        macd = ema12 - ema26
        features.append((macd / df['close']).fillna(0).values * 10)

        high_low = df['high'] - df['low']
        high_prevclose = np.abs(df['high'] - df['close'].shift(1))
        low_prevclose = np.abs(df['low'] - df['close'].shift(1))
        tr = pd.DataFrame({'hl': high_low, 'hpc': high_prevclose, 'lpc': low_prevclose}).max(axis=1)
        atr = tr.rolling(14).mean()
        features.append((atr / df['close']).fillna(0).values * 10)

        momentum = df['close'].diff(10) / df['close'].shift(10)
        features.append(momentum.fillna(0).values)

        features.append(df['close'].rolling(30).std().fillna(0).values / df['close'].mean())

        vol_ma10 = df['volume'].rolling(10).mean()
        features.append((df['volume'] / vol_ma10 - 1).fillna(0).values)

        high_10 = df['high'].rolling(10).max()
        low_10 = df['low'].rolling(10).min()
        features.append(((df['close'] - low_10) / (high_10 - low_10 + 1e-8)).fillna(0.5).values)

        typical_price = (df['high'] + df['low'] + df['close']) / 3
        features.append((typical_price / typical_price.shift(1) - 1).fillna(0).values)

        ma50 = df['close'].rolling(50).mean()
        features.append((df['close'] / ma50 - 1).fillna(0).values)

        for col in llm_cols:
            features.append(df[col].values * self.llm_weight)

        current_dim = len(features)
        if current_dim < INPUT_DIM:
            for _ in range(INPUT_DIM - current_dim):
                features.append(np.zeros(len(df)))

        features = features[:INPUT_DIM]

        return np.column_stack(features)

    def _create_targets(self, df):
        returns = []
        directions = []

        for h in range(1, self.pred_horizon + 1):
            if len(df['close']) <= h:
                ret = np.full(len(df), np.nan)
            else:
                ret = (df['close'].shift(-h) / df['close'] - 1).values

            returns.append(ret)
            directions.append((ret > 0).astype(float))

        return {
            'returns': np.column_stack(returns),
            'directions': np.column_stack(directions)
        }

    def _get_valid_indices(self):
        valid = []

        if self.features.size == 0:
            return []

        end_idx = len(self.features) - self.pred_horizon if self.is_train else len(self.features)

        for i in range(self.seq_len, end_idx):
            if not np.isnan(self.features[i - self.seq_len:i]).any():
                if self.is_train and self.targets:
                    if not np.isnan(self.targets['returns'][i]).any():
                        valid.append(i)
                else:
                    valid.append(i)
        return valid

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        i = self.valid_indices[idx]
        X = torch.FloatTensor(self.features[i - self.seq_len:i])

        if self.is_train and self.targets:
            y_returns = torch.FloatTensor(self.targets['returns'][i])
            y_directions = torch.FloatTensor(self.targets['directions'][i])
            return X, y_returns, y_directions
        else:
            return X, torch.zeros(self.pred_horizon), torch.zeros(self.pred_horizon)


class CombinedLoss(nn.Module):
    def __init__(self, alpha=0.5, beta=0.3, gamma=0.2):
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.mse = nn.MSELoss()
        self.bce = nn.BCELoss()

    def forward(self, pred_returns, pred_probs, true_returns, true_directions):
        loss_mse = self.mse(pred_returns, true_returns)
        loss_bce = self.bce(pred_probs, true_directions)

        pred_directions = (pred_returns > 0).float()
        loss_dir = 1.0 - (pred_directions == true_directions).float().mean()

        return self.alpha * loss_mse + self.beta * loss_bce + self.gamma * loss_dir


def train_model(model, train_loader, val_loader, epochs=15, lr=1e-4, patience=5):
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, verbose=True
    )
    criterion = CombinedLoss()
    mse_criterion = nn.MSELoss()
    bce_criterion = nn.BCELoss()
    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        train_batches = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs} (Train)")
        for X, y_returns, y_directions in pbar:
            X = X.to(device)
            y_returns = y_returns.to(device)
            y_directions = y_directions.to(device)

            optimizer.zero_grad()
            pred_returns, pred_probs = model(X)
            loss = criterion(pred_returns, pred_probs, y_returns, y_directions)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            train_loss += loss.item()
            train_batches += 1
            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})

        avg_train_loss = train_loss / train_batches

        model.eval()
        val_loss = 0.0
        val_mse_sum = 0.0
        val_bce_sum = 0.0
        val_dir_acc_sum = 0.0
        val_batches = 0

        with torch.no_grad():
            for X, y_returns, y_directions in val_loader:
                X = X.to(device)
                y_returns = y_returns.to(device)
                y_directions = y_directions.to(device)

                pred_returns, pred_probs = model(X)

                loss = criterion(pred_returns, pred_probs, y_returns, y_directions)
                val_loss += loss.item()

                val_mse_sum += mse_criterion(pred_returns, y_returns).item()
                val_bce_sum += bce_criterion(pred_probs, y_directions).item()
                pred_directions = (pred_returns > 0).float()
                val_dir_acc_sum += (pred_directions == y_directions).float().mean().item()

                val_batches += 1

        avg_val_loss = val_loss / val_batches
        avg_val_mse = val_mse_sum / val_batches
        avg_val_bce = val_bce_sum / val_batches
        avg_val_acc = val_dir_acc_sum / val_batches

        scheduler.step(avg_val_loss)

        print(
            f"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val MSE: {avg_val_mse:.5f} | Val BCE: {avg_val_bce:.4f} | Val Acc: {avg_val_acc:.4f}")

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'best_model.pt')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch + 1}")
                break

    if os.path.exists('best_model.pt'):
        model.load_state_dict(torch.load('best_model.pt'))
    return model


extractor = NewsFeatureExtractor(
    NEWS, CANDLES, TICKER_KEYWORDS,
    TRAIN_CUTOFF, OPENROUTE_API_KEY
)
LLM_FEATURES = extractor.extract_features(train_tickers=TRAIN_TICKERS)

llm_rows = []
for ticker, results_list in LLM_FEATURES.items():
    for i, (sentiment, impact) in enumerate(results_list):
        date = extractor.log_records[i]['date'] if i < len(extractor.log_records) else None
        llm_rows.append({
            'ticker': ticker,
            'begin': date,
            'sentiment': sentiment,
            'impact': impact
        })

LLM_FEATURES_DF = pd.DataFrame(llm_rows)
LLM_FEATURES_DF['begin'] = pd.to_datetime(LLM_FEATURES_DF['begin'])

ticker_data = {}
for ticker in TRAIN_TICKERS:
    df_ticker = CANDLES[CANDLES['ticker'] == ticker].copy()
    llm_ticker = LLM_FEATURES_DF[LLM_FEATURES_DF['ticker'] == ticker].copy()

    df_train = df_ticker[df_ticker['begin'] <= TRAIN_CUTOFF].copy()
    val_start = TRAIN_CUTOFF - pd.Timedelta(days=INPUT_WINDOW)
    df_val = df_ticker[df_ticker['begin'] >= val_start].copy()

    ticker_data[ticker] = {
        'train': df_train,
        'val': df_val,
        'llm': llm_ticker
    }

val_window = 200

train_datasets = []
val_datasets = []
feature_scaler = None

for i, ticker in enumerate(TRAIN_TICKERS):
    data = ticker_data[ticker]

    if len(data['train']) < INPUT_WINDOW + PRED_HORIZON:
        continue

    df_train = data['train'].iloc[:-val_window]
    train_ds = ImprovedTimeSeriesDataset(
        df_train,
        data['llm'],
        seq_len=INPUT_WINDOW,
        pred_horizon=PRED_HORIZON,
        is_train=True
    )
    if i == 0:
        feature_scaler = train_ds.feature_scaler
    if len(train_ds) > 0:
        train_datasets.append(train_ds)

    df_val = data['train'].iloc[-val_window:]
    val_ds = ImprovedTimeSeriesDataset(
        df_val,
        data['llm'],
        seq_len=INPUT_WINDOW,
        pred_horizon=PRED_HORIZON,
        feature_scaler=feature_scaler,
        is_train=True
    )
    if len(val_ds) > 0:
        val_datasets.append(val_ds)

train_dataset = ConcatDataset(train_datasets)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

val_dataset = ConcatDataset(val_datasets) if val_datasets else None
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False) if val_dataset else None

print(f"Train samples: {len(train_dataset)}")
print(f"Val samples: {len(val_dataset) if val_dataset else 0}")

print("\n –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")

model = ImprovedForecastModel(
    input_dim=INPUT_DIM,
    pred_horizon=PRED_HORIZON
).to(device)

print(f"Model on: {device}")

trained_model = train_model(
    model,
    train_loader,
    val_loader,
    epochs=10,
    lr=5e-4,
    patience=5
)

print("\n –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")


def generate_forecast(model, df_ticker, llm_ticker, feature_scaler, date_t, clip_pct=0.05, smooth_window=3):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ PRED_HORIZON –¥–Ω–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ INPUT_WINDOW –¥–Ω–µ–π.
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤.
    """
    model.eval()

    df_cut = df_ticker[df_ticker['begin'] <= date_t].copy()
    if len(df_cut) < INPUT_WINDOW:
        return {'R': np.zeros(PRED_HORIZON), 'P': np.zeros(PRED_HORIZON), 'prices': np.zeros(PRED_HORIZON)}

    ds = ImprovedTimeSeriesDataset(
        df_cut,
        llm_ticker,
        seq_len=INPUT_WINDOW,
        pred_horizon=PRED_HORIZON,
        feature_scaler=feature_scaler,
        is_train=False
    )

    if len(ds) == 0:
        return {'R': np.zeros(PRED_HORIZON), 'P': np.zeros(PRED_HORIZON), 'prices': np.zeros(PRED_HORIZON)}

    X, _, _ = ds[-1]
    X = X.unsqueeze(0).to(device)

    with torch.no_grad():
        pred_log_returns, pred_probs = model(X)

    pred_log_returns = pred_log_returns.cpu().numpy().flatten()

    if smooth_window > 1:
        pred_log_returns = pd.Series(pred_log_returns).rolling(window=smooth_window, min_periods=1,
                                                               center=True).mean().values

    pred_log_returns = np.clip(pred_log_returns, -clip_pct, clip_pct)

    last_price = df_cut['close'].iloc[-1]
    pred_prices = last_price * np.exp(np.cumsum(pred_log_returns))

    return {
        'R': pred_log_returns,
        'P': pred_probs.cpu().numpy().flatten(),
        'prices': pred_prices
    }


all_train_tickers = TRAIN_TICKERS
submission_rows = []

for ticker in tqdm(all_train_tickers, desc="Generating forecasts"):
    df_ticker = CANDLES[CANDLES['ticker'] == ticker].copy()
    llm_ticker = LLM_FEATURES_DF[LLM_FEATURES_DF['ticker'] == ticker].copy()

    df_cut = df_ticker[df_ticker['begin'] <= TRAIN_CUTOFF].copy()

    if len(df_cut) < INPUT_WINDOW:
        R = np.zeros(PRED_HORIZON)
    else:
        forecast = generate_forecast(
            trained_model,
            df_cut,
            llm_ticker,
            feature_scaler,
            TRAIN_CUTOFF
        )
        daily_log_r = forecast['R']
        R = np.exp(np.cumsum(daily_log_r)) - 1

    submission_rows.append([ticker] + R.tolist())

columns = ['ticker'] + [f'p{i + 1}' for i in range(PRED_HORIZON)]
submission_df = pd.DataFrame(submission_rows, columns=columns).set_index('ticker')

print(f"\n –ü—Ä–æ–≥–Ω–æ–∑—ã –≥–æ—Ç–æ–≤—ã –¥–ª—è {len(submission_df)} —Ç–∏–∫–µ—Ä–æ–≤")
print(submission_df.head())

submission_df.to_csv('forecast_results.csv', index=True)
print(f" –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: forecast_results.csv")
